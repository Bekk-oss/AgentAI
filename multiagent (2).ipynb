{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH2-XIwjHbjx",
        "outputId": "7b5523f0-fe8f-4c38-cccb-a58804ec40ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Google API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß Processing subtask 1: Design the API endpoints:  `/submit_data` (POST), `/predict` (POST), `/metrics` (GET).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code has a critical dependency issue and several areas needing improvement.\n",
            "‚ùå Critical issues: [\"The code fails to run due to a missing dependency ('flask_restful').  The test environment needs to be configured correctly to include this library.\", \"Hardcoded column names ('x', 'y') make the code inflexible and difficult to maintain.  These should be configurable parameters.\", 'Error handling is too generic.  The `except Exception as e` block catches all exceptions, masking potential underlying issues.  More specific exception handling is needed.', 'The model training is embedded in the main application logic. This should be separated into a dedicated training script to improve maintainability and reproducibility.', 'The data loading and saving uses pandas `to_csv`, which can be inefficient for large datasets. Consider using a more efficient database or data storage solution.', \"The `Metrics` class currently only calculates the mean of 'x' and 'y'.  More comprehensive metrics should be included to provide a better understanding of model performance.\", \"The code lacks input validation beyond checking for the presence and type of the 'x' column.  More robust validation is needed to handle edge cases and prevent unexpected errors.\"]\n",
            "\n",
            "üîß Processing subtask 2: Define the data schema for historical stock data (e.g., using Pydantic for input validation).  Include fields like date, open, high, low, close, volume, etc.  Handle missing data gracefully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "‚úÖ Tests passed\n",
            "\n",
            "üîß Processing subtask 3: Implement input validation for all API endpoints using Pydantic or similar.  Return appropriate HTTP error codes (e.g., 400 Bad Request) for invalid input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "‚úÖ Tests passed\n",
            "\n",
            "üîß Processing subtask 4: Develop the `/submit_data` endpoint. This endpoint should accept historical stock data in JSON format, validate the data, and store it in a persistent database (e.g., PostgreSQL, MongoDB). Implement error handling for database operations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code has a good structure and handles errors reasonably well. However, it lacks crucial aspects like database schema creation and robust input validation, and the timeout during testing suggests performance issues or environment problems.\n",
            "‚ùå Critical issues: ['The test timed out, indicating a potential problem with database connection, slow query execution, or a missing database setup.  The code should include explicit database schema creation and checks to ensure the database is accessible and the table exists before attempting insertion.', 'The input validation is insufficient.  It only checks for the presence of fields and performs basic type checking.  More rigorous validation is needed to ensure data integrity (e.g., date format validation, range checks for price and volume).', 'Error handling could be improved by providing more specific error messages to the client.  Generic error messages like \"Database error\" are not helpful for debugging.', 'The use of `debug=True` in `app.run()` is a security risk in a production environment. This should be removed or replaced with a proper logging and monitoring solution.', 'The code uses environment variables for database credentials, which is good practice, but it should include better handling of cases where these variables are not set.']\n",
            "\n",
            "üîß Processing subtask 5: Develop the prediction model (e.g., using scikit-learn, TensorFlow, PyTorch).  Choose an appropriate model based on the nature of the prediction task (regression, classification).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "‚úÖ Tests passed\n",
            "\n",
            "üîß Processing subtask 6: Train the prediction model using the historical data stored in the database. Implement model versioning to track different model iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "‚úÖ Tests passed\n",
            "\n",
            "üîß Processing subtask 7: Develop the `/predict` endpoint. This endpoint should accept new data in JSON format, validate it, use the trained model to generate predictions, and return the predictions in JSON format. Implement error handling for model prediction failures.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code is well-structured and includes comprehensive error handling.  However, there are areas for improvement in terms of clarity and robustness.\n",
            "‚ùå Critical issues: [\"The input validation only checks for the presence of keys, not their data types or values.  Add validation to ensure that 'feature1' and 'feature2' are of the correct type (e.g., numeric) and within an acceptable range.  Consider using a schema validation library like Marshmallow for more complex input validation.\", 'The error handling is too broad.  The `except Exception as e` blocks catch too many different types of errors, making debugging difficult.  Be more specific in the `except` clauses to catch only the expected errors.  Log the unexpected errors for better debugging.', \"The model loading should include a check for the model's version or other metadata to ensure that the correct model is loaded.  This is crucial for preventing deployment issues.\", 'The code assumes a single prediction output.  Add checks to handle cases where the model might produce multiple predictions or an array of predictions.  Clarify this in the docstring.', 'The use of `debug=True` in `app.run()` is a security risk in production.  Remove this for production deployments.', 'The code lacks logging.  Add logging to track requests, errors, and predictions for monitoring and debugging purposes.', 'The docstring for the `predict` function could be more descriptive, specifying the expected input format and the format of the prediction output.']\n",
            "\n",
            "üîß Processing subtask 8: Implement model performance metrics calculation (e.g., RMSE, MAE, accuracy). Store these metrics in a database or a file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "‚úÖ Tests passed\n",
            "\n",
            "üîß Processing subtask 9: Develop the `/metrics` endpoint. This endpoint should retrieve and return the model performance metrics in JSON format.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code is well-structured and includes good error handling. However, the test failure indicates a deployment issue, not a code problem.\n",
            "‚ùå Critical issues: ['The test failure is due to port 5000 being in use, not a code bug.  The code should be run on a different port or the conflicting process should be stopped.']\n",
            "\n",
            "üîß Processing subtask 10: Implement comprehensive logging throughout the API to track requests, errors, and performance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code is well-structured and uses appropriate logging practices.  However, there's a critical deployment issue and a minor style concern.\n",
            "‚ùå Critical issues: ['The test results indicate a port conflict.  The application should handle this gracefully, perhaps by allowing the port to be specified as a command-line argument or configuration setting.  Running with `debug=True` should not be done in production.', 'The logging statements within the `before_request` and `after_request` functions could potentially log sensitive data (e.g., request headers, request data).  Consider filtering sensitive information before logging, especially in a production environment.']\n",
            "\n",
            "üîß Processing subtask 11: Implement robust error handling and exception management across all endpoints. Return informative error messages to the client.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code is well-structured and handles exceptions appropriately.  Error handling is good, with specific error messages returned for different exception types. However, the test failure indicates a deployment issue, not a code problem.\n",
            "‚ùå Critical issues: ['The test failure is due to port 5000 being in use.  This is not a code bug; the server needs to be started on a different port (e.g., using the `port` argument in `app.run()`), or the conflicting process needs to be stopped.']\n",
            "\n",
            "üîß Processing subtask 12: Write unit and integration tests to ensure the correctness and reliability of the API and the prediction model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code has several critical issues preventing successful execution.  The API tests are failing due to incorrect assumptions about the API response (status code, content type, and JSON structure). The integration test, while using mocking, still relies on the actual API endpoint which is problematic. The prediction model tests pass but are insufficiently robust.\n",
            "‚ùå Critical issues: ['The API endpoint `http://localhost:5000/predict` is likely not running or not configured correctly.  The tests are failing with 404 (Not Found) and the response is not valid JSON.  The tests should be made more robust to handle potential network errors and unexpected responses.', 'The `test_api_response_content_type` test fails because the API is not returning `application/json` as expected. The API needs to be fixed to return the correct content type.', 'The `test_api_response_data` test fails because the API response is not valid JSON.  The API needs to be fixed to return valid JSON.', \"The integration test `test_integration_with_model` mocks `requests.post` but still calls the actual API endpoint within the `predict` function. This makes the test unreliable and dependent on the external API's availability. The `predict` function should be completely independent of external calls during testing.\", \"The `TestPredictionModel` class has insufficient test coverage.  It should include tests with a wider range of valid and invalid inputs to ensure the model's robustness.\", 'Error handling is insufficient. The code should include more comprehensive error handling for network issues, invalid JSON responses, and other potential exceptions.']\n",
            "\n",
            "üîß Processing subtask 13: Deploy the Flask API to a production environment (e.g., using Docker, Kubernetes, AWS, GCP). Consider using a reverse proxy (e.g., Nginx) for load balancing and security.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "‚úÖ Tests passed\n",
            "\n",
            "üîß Processing subtask 14: Implement security best practices, including input sanitization, authentication, and authorization (if needed).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running tests...\n",
            "üîç Reviewing failed tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Review feedback: The code has a good structure and attempts to address security concerns, but has critical flaws and needs significant improvements.\n",
            "‚ùå Critical issues: ['**Hardcoded Credentials:**  The `ALLOWED_USERS` dictionary contains hardcoded credentials. This is a major security vulnerability.  Credentials should be stored securely (e.g., using a database with strong password hashing and salting).', '**Weak Sanitization:** The `sanitize_input` function provides only basic HTML escaping.  This is insufficient to prevent XSS and SQL injection attacks in a real-world application. A robust input validation and sanitization library should be used.', '**Insufficient Authorization:** The `authorize` function is rudimentary and lacks a proper authorization mechanism. A more sophisticated system (e.g., Role-Based Access Control (RBAC)) is needed to manage user permissions effectively.', \"**Error Handling:** While the code includes some error handling, it's not comprehensive.  More robust error handling is needed to prevent unexpected crashes and provide informative error messages to the user.\", '**Debug Mode:** The server is running in debug mode (`debug=True`). This should be disabled in production to prevent security risks.', '**Secret Key Management:** The secret key is either taken from the environment or defaults to a placeholder.  While better than hardcoding, a more robust key management strategy is needed for production.', \"**HMAC Implementation:** While using HMAC is a step in the right direction, it's crucial to use a strong, randomly generated salt for each password.  The current implementation is vulnerable to rainbow table attacks if the same key is used for all users.\", '**Port Conflict:** The test results indicate a port conflict. The application should handle this gracefully, perhaps by allowing the port to be specified as a command-line argument or configuration option.']\n",
            "\n",
            "‚úÖ # Subtask 1: Design the API endpoints:  `/submit_data` (POST), `/predict` (POST), `/metrics` (GET).\n",
            "\n",
            "from flask import Flask, request, jsonify\n",
            "from flask_restful import reqparse, Api, Resource\n",
            "import pandas as pd\n",
            "from sklearn.linear_model import LinearRegression #Example model, replace as needed\n",
            "import joblib #For model persistence\n",
            "import os\n",
            "\n",
            "app = Flask(__name__)\n",
            "api = Api(app)\n",
            "\n",
            "# Configuration (replace with your actual paths)\n",
            "MODEL_PATH = \"model.pkl\"\n",
            "DATA_PATH = \"data.csv\"\n",
            "\n",
            "# Load model if it exists, otherwise train a new one (example)\n",
            "if os.path.exists(MODEL_PATH):\n",
            "    model = joblib.load(MODEL_PATH)\n",
            "else:\n",
            "    #Example data and model training. Replace with your actual data and model.\n",
            "    data = pd.DataFrame({'x': [1, 2, 3, 4, 5], 'y': [2, 4, 5, 4, 5]})\n",
            "    model = LinearRegression()\n",
            "    model.fit(data[['x']], data['y'])\n",
            "    joblib.dump(model, MODEL_PATH)\n",
            "\n",
            "\n",
            "class SubmitData(Resource):\n",
            "    def post(self):\n",
            "        try:\n",
            "            parser = reqparse.RequestParser()\n",
            "            parser.add_argument('data', type=list, required=True, location='json')\n",
            "            args = parser.parse_args()\n",
            "            new_data = pd.DataFrame(args['data'])\n",
            "            #Validate data - check for correct columns, data types etc.\n",
            "            if not all(col in new_data.columns for col in ['x']): #Replace 'x' with your actual column name\n",
            "                return jsonify({'error': 'Missing required column'}), 400\n",
            "            if not pd.api.types.is_numeric_dtype(new_data['x']): #Replace 'x' with your actual column name\n",
            "                return jsonify({'error': 'Incorrect data type'}), 400\n",
            "\n",
            "            #Append to existing data (replace with your data handling logic)\n",
            "            existing_data = pd.read_csv(DATA_PATH) if os.path.exists(DATA_PATH) else pd.DataFrame(columns=['x','y']) #Replace 'x', 'y' with your actual column names\n",
            "            combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
            "            combined_data.to_csv(DATA_PATH, index=False)\n",
            "            return jsonify({'message': 'Data submitted successfully'}), 200\n",
            "        except Exception as e:\n",
            "            return jsonify({'error': str(e)}), 500\n",
            "\n",
            "\n",
            "class Predict(Resource):\n",
            "    def post(self):\n",
            "        try:\n",
            "            parser = reqparse.RequestParser()\n",
            "            parser.add_argument('input', type=dict, required=True, location='json')\n",
            "            args = parser.parse_args()\n",
            "            input_data = pd.DataFrame([args['input']])\n",
            "            #Validate input data - check for correct columns, data types etc.\n",
            "            if not all(col in input_data.columns for col in ['x']): #Replace 'x' with your actual column name\n",
            "                return jsonify({'error': 'Missing required column'}), 400\n",
            "            if not pd.api.types.is_numeric_dtype(input_data['x']): #Replace 'x' with your actual column name\n",
            "                return jsonify({'error': 'Incorrect data type'}), 400\n",
            "\n",
            "            prediction = model.predict(input_data)\n",
            "            return jsonify({'prediction': prediction.tolist()}), 200\n",
            "        except Exception as e:\n",
            "            return jsonify({'error': str(e)}), 500\n",
            "\n",
            "\n",
            "class Metrics(Resource):\n",
            "    def get(self):\n",
            "        # Add your metrics calculation here.  Example:\n",
            "        try:\n",
            "            #Example metrics - replace with your actual metrics\n",
            "            data = pd.read_csv(DATA_PATH)\n",
            "            mean_x = data['x'].mean()\n",
            "            mean_y = data['y'].mean()\n",
            "            return jsonify({'mean_x': mean_x, 'mean_y': mean_y}), 200\n",
            "        except Exception as e:\n",
            "            return jsonify({'error': str(e)}), 500\n",
            "\n",
            "\n",
            "api.add_resource(SubmitData, '/submit_data')\n",
            "api.add_resource(Predict, '/predict')\n",
            "api.add_resource(Metrics, '/metrics')\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 2: Define the data schema for historical stock data (e.g., using Pydantic for input validation).  Include fields like date, open, high, low, close, volume, etc.  Handle missing data gracefully.\n",
            "\n",
            "from datetime import date\n",
            "from typing import Optional\n",
            "from pydantic import BaseModel, validator, ValidationError\n",
            "\n",
            "\n",
            "class HistoricalStockData(BaseModel):\n",
            "    date: date\n",
            "    open: Optional[float] = None\n",
            "    high: Optional[float] = None\n",
            "    low: Optional[float] = None\n",
            "    close: Optional[float] = None\n",
            "    volume: Optional[int] = None\n",
            "    adjusted_close: Optional[float] = None\n",
            "\n",
            "    @validator(\"*\", pre=True)\n",
            "    def handle_missing_data(cls, value):\n",
            "        if value is None or value == \"\" or value == \"null\" or value == \"NULL\":  #Handles various missing data representations\n",
            "            return None\n",
            "        return value\n",
            "\n",
            "    @validator('open', 'high', 'low', 'close', 'adjusted_close')\n",
            "    def check_positive_price(cls, v):\n",
            "        if v is not None and v < 0:\n",
            "            raise ValueError(\"Price values cannot be negative.\")\n",
            "        return v\n",
            "\n",
            "    @validator('volume')\n",
            "    def check_non_negative_volume(cls, v):\n",
            "        if v is not None and v < 0:\n",
            "            raise ValueError(\"Volume cannot be negative.\")\n",
            "        return v\n",
            "\n",
            "\n",
            "#Example Usage\n",
            "valid_data = {\n",
            "    \"date\": date(2024, 3, 8),\n",
            "    \"open\": 150.00,\n",
            "    \"high\": 152.50,\n",
            "    \"low\": 148.75,\n",
            "    \"close\": 151.25,\n",
            "    \"volume\": 1000000,\n",
            "    \"adjusted_close\":151.25\n",
            "}\n",
            "\n",
            "invalid_data_missing = {\n",
            "    \"date\": date(2024, 3, 9)\n",
            "}\n",
            "\n",
            "invalid_data_negative = {\n",
            "    \"date\": date(2024, 3, 10),\n",
            "    \"open\": 150.00,\n",
            "    \"high\": 152.50,\n",
            "    \"low\": -148.75,  #Negative low\n",
            "    \"close\": 151.25,\n",
            "    \"volume\": 1000000,\n",
            "    \"adjusted_close\":151.25\n",
            "}\n",
            "\n",
            "invalid_data_string = {\n",
            "    \"date\": date(2024, 3, 10),\n",
            "    \"open\": \"abc\", #Invalid Open price\n",
            "    \"high\": 152.50,\n",
            "    \"low\": 148.75,\n",
            "    \"close\": 151.25,\n",
            "    \"volume\": 1000000,\n",
            "    \"adjusted_close\":151.25\n",
            "}\n",
            "\n",
            "\n",
            "try:\n",
            "    stock_data = HistoricalStockData(**valid_data)\n",
            "    print(\"Valid data:\", stock_data)\n",
            "except ValidationError as e:\n",
            "    print(\"Validation Error (Valid Data):\", e)\n",
            "\n",
            "\n",
            "try:\n",
            "    stock_data = HistoricalStockData(**invalid_data_missing)\n",
            "    print(\"Invalid data (missing):\", stock_data)\n",
            "except ValidationError as e:\n",
            "    print(\"Validation Error (Missing Data):\", e)\n",
            "\n",
            "try:\n",
            "    stock_data = HistoricalStockData(**invalid_data_negative)\n",
            "    print(\"Invalid data (negative):\", stock_data)\n",
            "except ValidationError as e:\n",
            "    print(\"Validation Error (Negative Data):\", e)\n",
            "\n",
            "try:\n",
            "    stock_data = HistoricalStockData(**invalid_data_string)\n",
            "    print(\"Invalid data (string):\", stock_data)\n",
            "except ValidationError as e:\n",
            "    print(\"Validation Error (String Data):\", e)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 3: Implement input validation for all API endpoints using Pydantic or similar.  Return appropriate HTTP error codes (e.g., 400 Bad Request) for invalid input.\n",
            "\n",
            "from fastapi import FastAPI, HTTPException\n",
            "from pydantic import BaseModel, ValidationError, Field, validator\n",
            "\n",
            "app = FastAPI()\n",
            "\n",
            "class Item(BaseModel):\n",
            "    name: str = Field(..., min_length=1, max_length=50)  # ... means required\n",
            "    description: str | None = None\n",
            "    price: float = Field(..., gt=0)\n",
            "    tax: float | None = None\n",
            "\n",
            "    @validator('tax')\n",
            "    def tax_must_be_positive(cls, v):\n",
            "        if v is not None and v < 0:\n",
            "            raise ValueError(\"Tax must be non-negative\")\n",
            "        return v\n",
            "\n",
            "\n",
            "@app.post(\"/items/\", response_model=Item)\n",
            "async def create_item(item: Item):\n",
            "    \"\"\"\n",
            "    Creates a new item.  Input validation is handled by Pydantic.\n",
            "    \"\"\"\n",
            "    return item\n",
            "\n",
            "\n",
            "@app.post(\"/items_optional/\", response_model=Item)\n",
            "async def create_item_optional(item: Item):\n",
            "    \"\"\"\n",
            "    Creates a new item with optional fields.\n",
            "    \"\"\"\n",
            "    return item\n",
            "\n",
            "\n",
            "@app.post(\"/items_complex/\")\n",
            "async def create_item_complex(item: Item):\n",
            "    \"\"\"\n",
            "    Demonstrates handling of validation errors.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        #Explicit validation - useful for more complex scenarios beyond basic model definition\n",
            "        item.model_validate()\n",
            "        return item\n",
            "    except ValidationError as e:\n",
            "        raise HTTPException(status_code=400, detail=e.errors())\n",
            "\n",
            "\n",
            "class User(BaseModel):\n",
            "    username: str\n",
            "    email: str\n",
            "\n",
            "\n",
            "@app.post(\"/users/\")\n",
            "async def create_user(user: User):\n",
            "    \"\"\"\n",
            "    Creates a new user. Demonstrates a simple model.\n",
            "    \"\"\"\n",
            "    return user\n",
            "\n",
            "\n",
            "@app.exception_handler(ValidationError)\n",
            "async def validation_exception_handler(request, exc):\n",
            "    \"\"\"\n",
            "    A custom exception handler for Pydantic ValidationErrors.  Provides more detailed error messages.\n",
            "    \"\"\"\n",
            "    return HTTPException(status_code=422, detail=exc.errors())\n",
            "\n",
            "\n",
            "#Example of handling a specific error type\n",
            "@app.exception_handler(ValueError)\n",
            "async def value_error_handler(request, exc):\n",
            "    return HTTPException(status_code=400, detail=str(exc))\n",
            "\n",
            "\n",
            "#Example of a more robust error handling approach\n",
            "@app.post(\"/items_robust/\")\n",
            "async def create_item_robust(item_data: dict):\n",
            "    try:\n",
            "        item = Item(**item_data)\n",
            "        return item\n",
            "    except ValidationError as e:\n",
            "        errors = e.errors()\n",
            "        error_messages = [f\"{error['loc'][1]}: {error['msg']}\" for error in errors]\n",
            "        raise HTTPException(status_code=400, detail=\"\\n\".join(error_messages))\n",
            "    except Exception as e:\n",
            "        raise HTTPException(status_code=500, detail=f\"Internal Server Error: {e}\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 4: Develop the `/submit_data` endpoint. This endpoint should accept historical stock data in JSON format, validate the data, and store it in a persistent database (e.g., PostgreSQL, MongoDB). Implement error handling for database operations.\n",
            "\n",
            "import json\n",
            "from flask import Flask, request, jsonify\n",
            "import psycopg2 #Example using PostgreSQL.  Change for other DBs.\n",
            "from psycopg2 import Error\n",
            "import os\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "# Database configuration (replace with your credentials)\n",
            "DB_HOST = os.environ.get(\"DB_HOST\", \"localhost\")\n",
            "DB_NAME = os.environ.get(\"DB_NAME\", \"stock_data\")\n",
            "DB_USER = os.environ.get(\"DB_USER\", \"your_db_user\")\n",
            "DB_PASSWORD = os.environ.get(\"DB_PASSWORD\", \"your_db_password\")\n",
            "DB_PORT = os.environ.get(\"DB_PORT\", \"5432\")\n",
            "\n",
            "\n",
            "#Schema Validation (Adjust to your needs)\n",
            "def validate_data(data):\n",
            "    \"\"\"Validates the incoming JSON data.\"\"\"\n",
            "    required_fields = [\"symbol\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
            "    if not all(field in data for field in required_fields):\n",
            "        return False, \"Missing required fields\"\n",
            "    try:\n",
            "        #More robust type checking can be added here.\n",
            "        float(data[\"open\"])\n",
            "        float(data[\"high\"])\n",
            "        float(data[\"low\"])\n",
            "        float(data[\"close\"])\n",
            "        int(data[\"volume\"])\n",
            "    except (ValueError, TypeError):\n",
            "        return False, \"Invalid data types\"\n",
            "    return True, \"\"\n",
            "\n",
            "\n",
            "@app.route('/submit_data', methods=['POST'])\n",
            "def submit_data():\n",
            "    try:\n",
            "        data = request.get_json()\n",
            "        if not data:\n",
            "            return jsonify({\"error\": \"No JSON data provided\"}), 400\n",
            "\n",
            "        is_valid, error_message = validate_data(data)\n",
            "        if not is_valid:\n",
            "            return jsonify({\"error\": error_message}), 400\n",
            "\n",
            "        # Database interaction (PostgreSQL example)\n",
            "        conn = None\n",
            "        cur = None\n",
            "        try:\n",
            "            conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD, port=DB_PORT)\n",
            "            cur = conn.cursor()\n",
            "            cur.execute(\"\"\"\n",
            "                INSERT INTO stock_prices (symbol, date, open, high, low, close, volume)\n",
            "                VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
            "            \"\"\", (data['symbol'], data['date'], data['open'], data['high'], data['low'], data['close'], data['volume']))\n",
            "            conn.commit()\n",
            "            return jsonify({\"message\": \"Data submitted successfully\"}), 201\n",
            "        except (Exception, psycopg2.Error) as error:\n",
            "            if conn:\n",
            "                conn.rollback()\n",
            "            return jsonify({\"error\": f\"Database error: {error}\"}), 500\n",
            "        finally:\n",
            "            if cur:\n",
            "                cur.close()\n",
            "            if conn:\n",
            "                conn.close()\n",
            "\n",
            "    except json.JSONDecodeError:\n",
            "        return jsonify({\"error\": \"Invalid JSON format\"}), 400\n",
            "    except Exception as e:\n",
            "        return jsonify({\"error\": f\"An unexpected error occurred: {e}\"}), 500\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 5: Develop the prediction model (e.g., using scikit-learn, TensorFlow, PyTorch).  Choose an appropriate model based on the nature of the prediction task (regression, classification).\n",
            "\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
            "from sklearn.compose import ColumnTransformer\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "from sklearn.metrics import mean_squared_error, r2_score\n",
            "from sklearn.impute import SimpleImputer\n",
            "\n",
            "\n",
            "def create_and_train_model(data_path, target_column, categorical_cols=None, numerical_cols=None):\n",
            "    \"\"\"\n",
            "    Creates and trains a prediction model based on the provided data.\n",
            "\n",
            "    Args:\n",
            "        data_path (str): Path to the CSV data file.\n",
            "        target_column (str): Name of the target variable column.\n",
            "        categorical_cols (list, optional): List of categorical column names. Defaults to None.\n",
            "        numerical_cols (list, optional): List of numerical column names. Defaults to None.\n",
            "\n",
            "    Returns:\n",
            "        tuple: A tuple containing the trained model and the R-squared score on the test set.  Returns (None, None) if errors occur.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        # Load data\n",
            "        df = pd.read_csv(data_path)\n",
            "\n",
            "        #Handle missing data -  Simple imputation for demonstration.  More sophisticated methods should be used in production.\n",
            "        if numerical_cols:\n",
            "            imputer_num = SimpleImputer(strategy='mean')\n",
            "            df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])\n",
            "        if categorical_cols:\n",
            "            imputer_cat = SimpleImputer(strategy='most_frequent')\n",
            "            df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
            "\n",
            "\n",
            "        # Separate features and target\n",
            "        X = df.drop(target_column, axis=1)\n",
            "        y = df[target_column]\n",
            "\n",
            "        #Check for empty columns after imputation\n",
            "        if X.isnull().values.any():\n",
            "            raise ValueError(\"Missing values remain after imputation. Check your data and imputation strategy.\")\n",
            "\n",
            "        # Define preprocessing steps\n",
            "        if categorical_cols and numerical_cols:\n",
            "            preprocessor = ColumnTransformer(\n",
            "                transformers=[\n",
            "                    ('num', StandardScaler(), numerical_cols),\n",
            "                    ('cat', OneHotEncoder(), categorical_cols)\n",
            "                ])\n",
            "        elif categorical_cols:\n",
            "            preprocessor = ColumnTransformer(\n",
            "                transformers=[\n",
            "                    ('cat', OneHotEncoder(), categorical_cols)\n",
            "                ])\n",
            "        elif numerical_cols:\n",
            "            preprocessor = ColumnTransformer(\n",
            "                transformers=[\n",
            "                    ('num', StandardScaler(), numerical_cols)\n",
            "                ])\n",
            "        else:\n",
            "            raise ValueError(\"Must specify at least numerical or categorical columns.\")\n",
            "\n",
            "        # Create pipeline\n",
            "        model = Pipeline([\n",
            "            ('preprocessor', preprocessor),\n",
            "            ('regressor', RandomForestRegressor(random_state=42)) #Using RandomForestRegressor as a robust default.  Consider other models based on data.\n",
            "        ])\n",
            "\n",
            "        # Split data\n",
            "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
            "\n",
            "        # Train model\n",
            "        model.fit(X_train, y_train)\n",
            "\n",
            "        # Make predictions\n",
            "        y_pred = model.predict(X_test)\n",
            "\n",
            "        # Evaluate model\n",
            "        mse = mean_squared_error(y_test, y_pred)\n",
            "        r2 = r2_score(y_test, y_pred)\n",
            "\n",
            "        print(f\"Mean Squared Error: {mse}\")\n",
            "        print(f\"R-squared: {r2}\")\n",
            "\n",
            "        return model, r2\n",
            "\n",
            "    except FileNotFoundError:\n",
            "        print(f\"Error: File not found at {data_path}\")\n",
            "        return None, None\n",
            "    except pd.errors.EmptyDataError:\n",
            "        print(f\"Error: Data file is empty at {data_path}\")\n",
            "        return None, None\n",
            "    except ValueError as e:\n",
            "        print(f\"Error: {e}\")\n",
            "        return None, None\n",
            "    except Exception as e:\n",
            "        print(f\"An unexpected error occurred: {e}\")\n",
            "        return None, None\n",
            "\n",
            "\n",
            "\n",
            "# Example usage:  Replace with your data and column names.\n",
            "data_path = \"your_data.csv\" #Replace with your data file path\n",
            "target_column = \"target_variable\" #Replace with your target variable column name\n",
            "numerical_cols = ['numerical_col1', 'numerical_col2'] #Replace with your numerical column names\n",
            "categorical_cols = ['categorical_col1', 'categorical_col2'] #Replace with your categorical column names\n",
            "\n",
            "model, r2 = create_and_train_model(data_path, target_column, categorical_cols, numerical_cols)\n",
            "\n",
            "if model:\n",
            "    print(\"Model training successful!\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 6: Train the prediction model using the historical data stored in the database. Implement model versioning to track different model iterations.\n",
            "\n",
            "import sqlite3\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.metrics import accuracy_score\n",
            "import pickle\n",
            "import os\n",
            "from datetime import datetime\n",
            "\n",
            "\n",
            "def train_and_version_model(db_path, table_name, features, target, model_dir=\"models\"):\n",
            "    \"\"\"\n",
            "    Trains a prediction model using historical data from a SQLite database, \n",
            "    implements model versioning, and handles potential errors robustly.\n",
            "\n",
            "    Args:\n",
            "        db_path (str): Path to the SQLite database file.\n",
            "        table_name (str): Name of the table containing the data.\n",
            "        features (list): List of column names representing the features.\n",
            "        target (str): Name of the column representing the target variable.\n",
            "        model_dir (str, optional): Directory to store trained models. Defaults to \"models\".\n",
            "\n",
            "    Returns:\n",
            "        str: Path to the saved model file, or None if an error occurred.  \n",
            "             Prints informative error messages.\n",
            "    \"\"\"\n",
            "\n",
            "    if not os.path.exists(db_path):\n",
            "        print(f\"Error: Database file not found at {db_path}\")\n",
            "        return None\n",
            "\n",
            "    if not os.path.exists(model_dir):\n",
            "        os.makedirs(model_dir)\n",
            "\n",
            "    try:\n",
            "        conn = sqlite3.connect(db_path)\n",
            "        query = f\"SELECT {', '.join(features + [target])} FROM {table_name}\"\n",
            "        df = pd.read_sql_query(query, conn)\n",
            "        conn.close()\n",
            "\n",
            "        #Basic data validation.  More robust checks might be needed in a production setting.\n",
            "        if df.isnull().values.any():\n",
            "            print(\"Warning: Data contains missing values.  Consider imputation or removal.\")\n",
            "            df.dropna(inplace=True) #Simple removal for this example.  Better approaches exist.\n",
            "\n",
            "        X = df[features]\n",
            "        y = df[target]\n",
            "\n",
            "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
            "\n",
            "        model = LogisticRegression() #Example model.  Choose appropriate model for your data.\n",
            "        model.fit(X_train, y_train)\n",
            "\n",
            "        y_pred = model.predict(X_test)\n",
            "        accuracy = accuracy_score(y_test, y_pred)\n",
            "        print(f\"Model accuracy: {accuracy}\")\n",
            "\n",
            "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
            "        model_filename = os.path.join(model_dir, f\"model_v{timestamp}.pkl\")\n",
            "\n",
            "        with open(model_filename, 'wb') as file:\n",
            "            pickle.dump(model, file)\n",
            "\n",
            "        print(f\"Model saved to: {model_filename}\")\n",
            "        return model_filename\n",
            "\n",
            "    except sqlite3.Error as e:\n",
            "        print(f\"Error accessing database: {e}\")\n",
            "        return None\n",
            "    except pd.io.sql.DatabaseError as e:\n",
            "        print(f\"Error reading data from database: {e}\")\n",
            "        return None\n",
            "    except Exception as e:\n",
            "        print(f\"An unexpected error occurred: {e}\")\n",
            "        return None\n",
            "\n",
            "\n",
            "\n",
            "# Example usage:\n",
            "db_path = \"mydatabase.db\"  # Replace with your database path\n",
            "table_name = \"historical_data\"  # Replace with your table name\n",
            "features = ['feature1', 'feature2', 'feature3']  # Replace with your feature columns\n",
            "target = 'target_variable'  # Replace with your target column name\n",
            "\n",
            "trained_model_path = train_and_version_model(db_path, table_name, features, target)\n",
            "\n",
            "#To load and use a specific version:\n",
            "# with open(trained_model_path, 'rb') as f:\n",
            "#     loaded_model = pickle.load(f)\n",
            "# predictions = loaded_model.predict(new_data)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 7: Develop the `/predict` endpoint. This endpoint should accept new data in JSON format, validate it, use the trained model to generate predictions, and return the predictions in JSON format. Implement error handling for model prediction failures.\n",
            "\n",
            "import json\n",
            "import flask\n",
            "from flask import Flask, request, jsonify\n",
            "import numpy as np\n",
            "# Replace with your actual model loading\n",
            "import joblib #Example using joblib, adjust as needed for your model\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "# Load the pre-trained model.  Error handling crucial here.\n",
            "try:\n",
            "    model = joblib.load('trained_model.pkl') # Replace 'trained_model.pkl' with your model file\n",
            "    print(\"Model loaded successfully.\")\n",
            "except FileNotFoundError:\n",
            "    print(\"Error: Model file not found. Please ensure 'trained_model.pkl' exists.\")\n",
            "    exit(1)\n",
            "except Exception as e:\n",
            "    print(f\"Error loading model: {e}\")\n",
            "    exit(1)\n",
            "\n",
            "\n",
            "@app.route('/predict', methods=['POST'])\n",
            "def predict():\n",
            "    \"\"\"\n",
            "    Endpoint to receive data, make predictions, and return results.  Handles various error conditions.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        data = request.get_json()\n",
            "        if not data:\n",
            "            return jsonify({'error': 'No input data provided'}), 400\n",
            "\n",
            "        # Input validation.  Customize based on your model's input requirements.\n",
            "        required_keys = ['feature1', 'feature2'] #Example, replace with your features\n",
            "        if not all(key in data for key in required_keys):\n",
            "            return jsonify({'error': 'Missing required keys in input data'}), 400\n",
            "\n",
            "        try:\n",
            "            #Convert to numpy array, handle potential type errors\n",
            "            input_array = np.array([[data['feature1'], data['feature2']]]) #Example, adjust as needed\n",
            "\n",
            "        except (KeyError, ValueError) as e:\n",
            "            return jsonify({'error': f'Invalid input data format: {e}'}), 400\n",
            "\n",
            "\n",
            "        #Prediction.  Robust error handling is key.\n",
            "        try:\n",
            "            prediction = model.predict(input_array)\n",
            "            #Post-processing if needed (e.g., converting to specific format)\n",
            "            prediction = prediction.tolist()[0] #Example: Assuming single prediction output\n",
            "\n",
            "        except Exception as e:\n",
            "            return jsonify({'error': f'Model prediction failed: {e}'}), 500\n",
            "\n",
            "        return jsonify({'prediction': prediction})\n",
            "\n",
            "    except Exception as e:\n",
            "        return jsonify({'error': f'An unexpected error occurred: {e}'}), 500\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 8: Implement model performance metrics calculation (e.g., RMSE, MAE, accuracy). Store these metrics in a database or a file.\n",
            "\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
            "import sqlite3  # Using SQLite for database storage.  Could easily be swapped for other DBs.\n",
            "\n",
            "\n",
            "def calculate_metrics(y_true, y_pred, task_type=\"regression\", db_path=\"metrics.db\", table_name=\"model_metrics\"):\n",
            "    \"\"\"\n",
            "    Calculates regression or classification metrics and stores them in a database.\n",
            "\n",
            "    Args:\n",
            "        y_true: True labels (NumPy array or list).\n",
            "        y_pred: Predicted labels (NumPy array or list).\n",
            "        task_type: Type of task (\"regression\" or \"classification\"). Defaults to \"regression\".\n",
            "        db_path: Path to the SQLite database file. Defaults to \"metrics.db\".\n",
            "        table_name: Name of the table to store metrics in. Defaults to \"model_metrics\".\n",
            "\n",
            "    Raises:\n",
            "        ValueError: If task_type is invalid or input arrays have different lengths.\n",
            "        TypeError: if input is not a list or numpy array.\n",
            "\n",
            "    Returns:\n",
            "        A dictionary containing calculated metrics.  Returns None if there's a database error.\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)):\n",
            "        raise TypeError(\"y_true and y_pred must be lists or numpy arrays.\")\n",
            "\n",
            "    if len(y_true) != len(y_pred):\n",
            "        raise ValueError(\"y_true and y_pred must have the same length.\")\n",
            "\n",
            "    if task_type not in [\"regression\", \"classification\"]:\n",
            "        raise ValueError(\"task_type must be 'regression' or 'classification'.\")\n",
            "\n",
            "    metrics = {}\n",
            "    if task_type == \"regression\":\n",
            "        metrics[\"rmse\"] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
            "        metrics[\"mae\"] = mean_absolute_error(y_true, y_pred)\n",
            "    elif task_type == \"classification\":\n",
            "        metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
            "\n",
            "\n",
            "    try:\n",
            "        conn = sqlite3.connect(db_path)\n",
            "        cursor = conn.cursor()\n",
            "\n",
            "        # Create table if it doesn't exist.  Handles potential schema mismatches gracefully.\n",
            "        cursor.execute(f'''\n",
            "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
            "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
            "                rmse REAL,\n",
            "                mae REAL,\n",
            "                accuracy REAL\n",
            "            )\n",
            "        ''')\n",
            "\n",
            "        # Insert metrics.  Handles potential missing columns by only inserting available metrics.\n",
            "        insert_query = f\"INSERT INTO {table_name} (rmse, mae, accuracy) VALUES (?, ?, ?)\"\n",
            "        cursor.execute(insert_query, (metrics.get(\"rmse\"), metrics.get(\"mae\"), metrics.get(\"accuracy\")))\n",
            "\n",
            "        conn.commit()\n",
            "        conn.close()\n",
            "        return metrics\n",
            "    except sqlite3.Error as e:\n",
            "        print(f\"An error occurred: {e}\")\n",
            "        return None\n",
            "\n",
            "\n",
            "\n",
            "# Example usage:\n",
            "y_true_reg = [1, 2, 3, 4, 5]\n",
            "y_pred_reg = [1.1, 1.9, 3.2, 3.8, 5.1]\n",
            "\n",
            "y_true_class = [0, 1, 1, 0, 1]\n",
            "y_pred_class = [0, 1, 0, 0, 1]\n",
            "\n",
            "\n",
            "regression_metrics = calculate_metrics(y_true_reg, y_pred_reg, task_type=\"regression\")\n",
            "classification_metrics = calculate_metrics(y_true_class, y_pred_class, task_type=\"classification\")\n",
            "\n",
            "print(\"Regression Metrics:\", regression_metrics)\n",
            "print(\"Classification Metrics:\", classification_metrics)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 9: Develop the `/metrics` endpoint. This endpoint should retrieve and return the model performance metrics in JSON format.\n",
            "\n",
            "from flask import Flask, jsonify\n",
            "import json\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "# Sample model performance metrics.  Replace with your actual metric retrieval.\n",
            "model_metrics = {\n",
            "    \"accuracy\": 0.92,\n",
            "    \"precision\": 0.88,\n",
            "    \"recall\": 0.95,\n",
            "    \"f1_score\": 0.91,\n",
            "    \"AUC\": 0.97,\n",
            "    \"timestamp\": \"2024-10-27T10:30:00Z\" #Example timestamp, replace with your method\n",
            "}\n",
            "\n",
            "\n",
            "@app.route('/metrics')\n",
            "def get_metrics():\n",
            "    \"\"\"\n",
            "    Retrieves and returns model performance metrics in JSON format.  Handles potential errors gracefully.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        #In a real-world scenario, this would fetch metrics from a database, file, or other source.\n",
            "        #Error handling is crucial here to prevent crashes if the data source is unavailable.\n",
            "        #Example:  metrics = load_metrics_from_database()\n",
            "\n",
            "        #Check for valid metrics before returning\n",
            "        if not isinstance(model_metrics, dict):\n",
            "            return jsonify({\"error\": \"Invalid metrics format\"}), 500\n",
            "        if not all(key in model_metrics for key in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]):\n",
            "            return jsonify({\"error\": \"Missing required metrics\"}), 500\n",
            "\n",
            "        return jsonify(model_metrics)\n",
            "    except Exception as e:\n",
            "        # Log the error for debugging purposes.  In production, use a proper logging library.\n",
            "        print(f\"Error retrieving metrics: {e}\")\n",
            "        return jsonify({\"error\": \"Failed to retrieve metrics\"}), 500\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 10: Implement comprehensive logging throughout the API to track requests, errors, and performance.\n",
            "\n",
            "import logging\n",
            "import logging.handlers\n",
            "import time\n",
            "from flask import Flask, request, jsonify\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "# Configure logging\n",
            "log_level = logging.DEBUG  # Set to logging.INFO for production\n",
            "log_format = '%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'\n",
            "formatter = logging.Formatter(log_format)\n",
            "\n",
            "# Create handlers\n",
            "file_handler = logging.handlers.RotatingFileHandler('api.log', maxBytes=10*1024*1024, backupCount=5)\n",
            "file_handler.setLevel(log_level)\n",
            "file_handler.setFormatter(formatter)\n",
            "\n",
            "console_handler = logging.StreamHandler()\n",
            "console_handler.setLevel(log_level)\n",
            "console_handler.setFormatter(formatter)\n",
            "\n",
            "# Create logger\n",
            "logger = logging.getLogger(__name__)\n",
            "logger.setLevel(log_level)\n",
            "logger.addHandler(file_handler)\n",
            "logger.addHandler(console_handler)\n",
            "\n",
            "\n",
            "@app.before_request\n",
            "def before_request_logging():\n",
            "    \"\"\"Log request details before processing.\"\"\"\n",
            "    try:\n",
            "        request_data = request.get_json() if request.is_json else request.data\n",
            "        logger.info(f\"Request received: {request.method} {request.path} - Data: {request_data}\")\n",
            "        logger.debug(f\"Request headers: {request.headers}\")\n",
            "    except Exception as e:\n",
            "        logger.exception(f\"Error logging request details: {e}\")\n",
            "\n",
            "\n",
            "@app.after_request\n",
            "def after_request_logging(response):\n",
            "    \"\"\"Log response details after processing.\"\"\"\n",
            "    try:\n",
            "        logger.info(f\"Response sent: {response.status_code} - Data: {response.data}\")\n",
            "        response.headers['X-Response-Time'] = str(time.time() - request.start_time) #add response time\n",
            "    except Exception as e:\n",
            "        logger.exception(f\"Error logging response details: {e}\")\n",
            "    return response\n",
            "\n",
            "\n",
            "@app.route('/api/data', methods=['GET', 'POST'])\n",
            "def api_data():\n",
            "    \"\"\"Example API endpoint.\"\"\"\n",
            "    start_time = time.time()\n",
            "    request.start_time = start_time #add start time to request object\n",
            "    try:\n",
            "        if request.method == 'POST':\n",
            "            data = request.get_json()\n",
            "            if not data or 'value' not in data:\n",
            "                return jsonify({'error': 'Missing value'}), 400\n",
            "            result = data['value'] * 2\n",
            "            logger.info(f\"Processed data: {data}, Result: {result}\")\n",
            "            return jsonify({'result': result}), 200\n",
            "        else:\n",
            "            return jsonify({'message': 'GET request received'}), 200\n",
            "    except Exception as e:\n",
            "        logger.exception(f\"Error processing request: {e}\")\n",
            "        return jsonify({'error': 'Internal Server Error'}), 500\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 11: Implement robust error handling and exception management across all endpoints. Return informative error messages to the client.\n",
            "\n",
            "from flask import Flask, jsonify, request\n",
            "import traceback\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "@app.route('/endpoint1', methods=['POST'])\n",
            "def endpoint1():\n",
            "    try:\n",
            "        data = request.get_json()\n",
            "        if not data or 'key1' not in data:\n",
            "            raise ValueError(\"Missing 'key1' in request body\")\n",
            "        # ... your logic here ...\n",
            "        result = {'message': 'Endpoint 1 successful', 'data': data['key1']}\n",
            "        return jsonify(result), 200\n",
            "    except ValueError as e:\n",
            "        return jsonify({'error': str(e)}), 400  # Bad Request\n",
            "    except KeyError as e:\n",
            "        return jsonify({'error': f\"Missing key: {e}\"}), 400\n",
            "    except Exception as e:\n",
            "        error_message = f\"Internal Server Error: {type(e).__name__} - {str(e)}\"\n",
            "        app.logger.error(f\"Error in endpoint1: {error_message}\\n{traceback.format_exc()}\") # Log the full traceback for debugging\n",
            "        return jsonify({'error': \"Internal Server Error\"}), 500\n",
            "\n",
            "\n",
            "@app.route('/endpoint2', methods=['GET'])\n",
            "def endpoint2():\n",
            "    try:\n",
            "        param = request.args.get('param')\n",
            "        if not param:\n",
            "            raise ValueError(\"Missing 'param' query parameter\")\n",
            "        # ... your logic here ...  (e.g., database interaction)\n",
            "        if param == \"error\":\n",
            "            raise RuntimeError(\"Simulated database error\")\n",
            "        result = {'message': 'Endpoint 2 successful', 'param': param}\n",
            "        return jsonify(result), 200\n",
            "    except ValueError as e:\n",
            "        return jsonify({'error': str(e)}), 400\n",
            "    except RuntimeError as e:\n",
            "        return jsonify({'error': f\"Database error: {e}\"}), 500 # Specific error message\n",
            "    except Exception as e:\n",
            "        error_message = f\"Internal Server Error: {type(e).__name__} - {str(e)}\"\n",
            "        app.logger.error(f\"Error in endpoint2: {error_message}\\n{traceback.format_exc()}\")\n",
            "        return jsonify({'error': \"Internal Server Error\"}), 500\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 12: Write unit and integration tests to ensure the correctness and reliability of the API and the prediction model.\n",
            "\n",
            "import unittest\n",
            "import requests\n",
            "import json\n",
            "from unittest.mock import patch\n",
            "\n",
            "# Assume your API endpoint is at this URL.  Replace with your actual endpoint.\n",
            "API_ENDPOINT = \"http://localhost:5000/predict\"  \n",
            "\n",
            "# Sample data for testing. Replace with your actual data format.\n",
            "sample_input_data = {\"feature1\": 10, \"feature2\": 20}\n",
            "expected_output = {\"prediction\": 30} # Replace with your expected output\n",
            "\n",
            "\n",
            "class TestAPI(unittest.TestCase):\n",
            "\n",
            "    def test_api_response_status_code(self):\n",
            "        response = requests.post(API_ENDPOINT, json=sample_input_data)\n",
            "        self.assertEqual(response.status_code, 200, f\"Unexpected status code: {response.status_code}\")\n",
            "\n",
            "    def test_api_response_content_type(self):\n",
            "        response = requests.post(API_ENDPOINT, json=sample_input_data)\n",
            "        self.assertEqual(response.headers['Content-Type'], 'application/json')\n",
            "\n",
            "    def test_api_response_data(self):\n",
            "        response = requests.post(API_ENDPOINT, json=sample_input_data)\n",
            "        try:\n",
            "            data = response.json()\n",
            "            self.assertEqual(data, expected_output)\n",
            "        except json.JSONDecodeError:\n",
            "            self.fail(\"API response is not valid JSON\")\n",
            "\n",
            "    @patch('requests.post') # Mocking the external API call for integration test\n",
            "    def test_integration_with_model(self, mock_post):\n",
            "        # Simulate a successful API call with a mocked response.\n",
            "        mock_post.return_value.status_code = 200\n",
            "        mock_post.return_value.json.return_value = expected_output\n",
            "        \n",
            "        # Your prediction logic (replace with your actual prediction function)\n",
            "        def predict(input_data):\n",
            "            response = requests.post(API_ENDPOINT, json=input_data)\n",
            "            return response.json()\n",
            "\n",
            "        prediction = predict(sample_input_data)\n",
            "        self.assertEqual(prediction, expected_output)\n",
            "\n",
            "        #Simulate a failed API call\n",
            "        mock_post.return_value.status_code = 500\n",
            "        mock_post.return_value.json.side_effect = requests.exceptions.RequestException(\"Network Error\")\n",
            "        with self.assertRaises(requests.exceptions.RequestException):\n",
            "            predict(sample_input_data)\n",
            "\n",
            "\n",
            "#Example of a simple prediction model (replace with your actual model)\n",
            "def prediction_model(input_data):\n",
            "    return {\"prediction\": input_data[\"feature1\"] + input_data[\"feature2\"]}\n",
            "\n",
            "\n",
            "class TestPredictionModel(unittest.TestCase):\n",
            "    def test_model_prediction(self):\n",
            "        result = prediction_model(sample_input_data)\n",
            "        self.assertEqual(result, expected_output)\n",
            "\n",
            "    def test_model_with_invalid_input(self):\n",
            "        with self.assertRaises(KeyError): #or other appropriate exception\n",
            "            prediction_model({\"feature1\":10})\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 13: Deploy the Flask API to a production environment (e.g., using Docker, Kubernetes, AWS, GCP). Consider using a reverse proxy (e.g., Nginx) for load balancing and security.\n",
            "\n",
            "import subprocess\n",
            "\n",
            "def deploy_flask_app(app_name=\"flask_app\", dockerfile_path=\"Dockerfile\", kubernetes_yaml=\"kubernetes.yaml\", aws_region=\"us-west-2\"):\n",
            "    \"\"\"\n",
            "    Deploys a Flask application to a production environment using Docker, Kubernetes, and optionally AWS.\n",
            "\n",
            "    Args:\n",
            "        app_name: The name of the Flask application.  Must match Dockerfile and Kubernetes YAML.\n",
            "        dockerfile_path: Path to the Dockerfile. Defaults to \"Dockerfile\" in the current directory.\n",
            "        kubernetes_yaml: Path to the Kubernetes YAML deployment file. Defaults to \"kubernetes.yaml\".\n",
            "        aws_region: AWS region for deployment (only used if deploying to AWS EKS).\n",
            "\n",
            "    Raises:\n",
            "        subprocess.CalledProcessError: If any of the subprocess calls fail.\n",
            "        FileNotFoundError: If Dockerfile or Kubernetes YAML file is not found.\n",
            "        ValueError: If invalid arguments are provided.\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    if not app_name:\n",
            "        raise ValueError(\"app_name cannot be empty.\")\n",
            "\n",
            "    # Check for required files\n",
            "    try:\n",
            "        with open(dockerfile_path, \"r\"):\n",
            "            pass  # Check if file exists and is readable\n",
            "        with open(kubernetes_yaml, \"r\"):\n",
            "            pass\n",
            "    except FileNotFoundError as e:\n",
            "        raise FileNotFoundError(f\"Error: {e}.  Ensure Dockerfile and kubernetes.yaml exist.\")\n",
            "\n",
            "\n",
            "    # 1. Build the Docker image\n",
            "    try:\n",
            "        subprocess.run([\"docker\", \"build\", \"-t\", app_name, \".\"], check=True, cwd=dockerfile_path.rsplit('/',1)[0] if '/' in dockerfile_path else '.')\n",
            "        print(\"Docker image built successfully.\")\n",
            "    except subprocess.CalledProcessError as e:\n",
            "        raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr)\n",
            "\n",
            "\n",
            "    # 2. Deploy to Kubernetes (assuming kubectl is configured)\n",
            "    try:\n",
            "        subprocess.run([\"kubectl\", \"apply\", \"-f\", kubernetes_yaml], check=True)\n",
            "        print(\"Deployment to Kubernetes successful.\")\n",
            "    except subprocess.CalledProcessError as e:\n",
            "        raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr) from None\n",
            "        #Note:  More robust error handling could be added here to check for specific Kubernetes errors.\n",
            "\n",
            "\n",
            "    # 3. Optional: Deploy to AWS EKS (requires AWS CLI configured)\n",
            "    #  This section is commented out as it requires AWS-specific configuration and credentials.\n",
            "    #  Uncomment and adapt as needed for your AWS environment.\n",
            "\n",
            "    # try:\n",
            "    #     subprocess.run([\"aws\", \"eks\", \"update-kubeconfig\", \"--name\", \"your-eks-cluster-name\", \"--region\", aws_region], check=True)\n",
            "    #     subprocess.run([\"kubectl\", \"apply\", \"-f\", kubernetes_yaml], check=True)\n",
            "    #     print(\"Deployment to AWS EKS successful.\")\n",
            "    # except subprocess.CalledProcessError as e:\n",
            "    #     raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr) from None\n",
            "\n",
            "\n",
            "    print(\"Deployment complete.\")\n",
            "\n",
            "\n",
            "# Example usage (replace with your actual paths and settings):\n",
            "# Ensure you have Docker, kubectl, and potentially AWS CLI installed and configured.\n",
            "# A properly configured kubernetes.yaml file is crucial.  This example omits it for brevity.\n",
            "\n",
            "# deploy_flask_app(app_name=\"myflaskapp\", dockerfile_path=\"myflaskapp/Dockerfile\", kubernetes_yaml=\"myflaskapp/kubernetes.yaml\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Subtask 14: Implement security best practices, including input sanitization, authentication, and authorization (if needed).\n",
            "\n",
            "import hashlib\n",
            "import hmac\n",
            "import os\n",
            "from flask import Flask, request, jsonify\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "#  Configuration (In a real application, this would be in a separate config file)\n",
            "app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY') or 'REPLACE_WITH_A_STRONG_SECRET_KEY' # NEVER hardcode in production\n",
            "ALLOWED_USERS = {'user1': 'password123', 'user2': 'securepass'} # Replace with secure user management\n",
            "\n",
            "\n",
            "def authenticate(username, password):\n",
            "    \"\"\"Authenticates a user using HMAC for password hashing.\"\"\"\n",
            "    if username not in ALLOWED_USERS:\n",
            "        return False\n",
            "    stored_password = ALLOWED_USERS[username]\n",
            "    key = app.config['SECRET_KEY'].encode('utf-8')  # Use config for key\n",
            "    hashed_password = hmac.new(key, password.encode('utf-8'), hashlib.sha256).hexdigest()\n",
            "    return hashed_password == stored_password\n",
            "\n",
            "\n",
            "def sanitize_input(data):\n",
            "    \"\"\"Sanitizes user input to prevent common vulnerabilities like XSS and SQL injection.\"\"\"\n",
            "    if isinstance(data, dict):\n",
            "        return {k: sanitize_input(v) for k, v in data.items()}\n",
            "    elif isinstance(data, list):\n",
            "        return [sanitize_input(item) for item in data]\n",
            "    elif isinstance(data, str):\n",
            "        # Basic sanitization - replace with a more robust library for production\n",
            "        return data.replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;').replace(\"'\", '&#39;')\n",
            "    else:\n",
            "        return data\n",
            "\n",
            "\n",
            "def authorize(username, action):\n",
            "    \"\"\"Checks if a user has permission to perform a specific action.\"\"\"\n",
            "    # Replace with a more sophisticated authorization system in a real application (e.g., RBAC)\n",
            "    if username == 'user1' and action == 'read':\n",
            "        return True\n",
            "    elif username == 'user2' and action in ['read', 'write']:\n",
            "        return True\n",
            "    return False\n",
            "\n",
            "\n",
            "@app.route('/api/data', methods=['GET', 'POST'])\n",
            "def api_data():\n",
            "    auth = request.authorization\n",
            "    if not auth or not authenticate(auth.username, auth.password):\n",
            "        return jsonify({'error': 'Unauthorized'}), 401\n",
            "\n",
            "    if request.method == 'GET':\n",
            "        if authorize(auth.username, 'read'):\n",
            "            # Access and return data\n",
            "            data = {'message': 'Data retrieved successfully'}\n",
            "            return jsonify(sanitize_input(data)) #Sanitize the response as well\n",
            "        else:\n",
            "            return jsonify({'error': 'Forbidden'}), 403\n",
            "\n",
            "    elif request.method == 'POST':\n",
            "        if authorize(auth.username, 'write'):\n",
            "            try:\n",
            "                data = request.get_json()\n",
            "                sanitized_data = sanitize_input(data) #Sanitize the request\n",
            "                # Process the sanitized data\n",
            "                # ... your data processing logic here ...\n",
            "                return jsonify({'message': 'Data processed successfully'}), 201\n",
            "            except Exception as e:\n",
            "                return jsonify({'error': str(e)}), 400\n",
            "        else:\n",
            "            return jsonify({'error': 'Forbidden'}), 403\n",
            "\n",
            "    return jsonify({'error': 'Method not allowed'}), 405\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True) # Set debug=False in production\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "import os\n",
        "import json\n",
        "import getpass\n",
        "import tempfile\n",
        "import subprocess\n",
        "import re\n",
        "\n",
        "# Set up Gemini API key\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')  # Update secret name\n",
        "    except:\n",
        "        import getpass\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter Google API Key: \")\n",
        "\n",
        "class PlannerAgent:\n",
        "    def __init__(self):\n",
        "        # Initialize Gemini model\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash-latest\",\n",
        "            temperature=0.3,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        As an expert solution architect, decompose complex problems into executable sub-tasks.\n",
        "        Use this JSON structure:\n",
        "        {\n",
        "            \"subtasks\": [\n",
        "                {\n",
        "                    \"id\": <unique integer>,\n",
        "                    \"desc\": \"<clear description>\",\n",
        "                    \"dependencies\": [<list of prerequisite task IDs>]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "    def decompose(self, task):\n",
        "        try:\n",
        "            # Create message sequence\n",
        "            messages = [\n",
        "                SystemMessage(content=self.system_prompt),\n",
        "                HumanMessage(content=f\"TASK: {task}\")\n",
        "            ]\n",
        "\n",
        "            # Get structured JSON response\n",
        "            llm_response = self.llm.invoke(messages)\n",
        "\n",
        "            # Extract JSON content\n",
        "            response_content = llm_response.content\n",
        "\n",
        "            # Handle potential formatting variations\n",
        "            if '```json' in response_content:\n",
        "                json_match = re.search(r'```json(.*?)```', response_content, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_content = json_match.group(1).strip()\n",
        "\n",
        "            # Parse and validate response\n",
        "            plan = json.loads(response_content)\n",
        "\n",
        "            # Validate structure\n",
        "            if \"subtasks\" not in plan or not isinstance(plan[\"subtasks\"], list):\n",
        "                raise ValueError(\"Invalid response format: Missing 'subtasks' list\")\n",
        "\n",
        "            for subtask in plan[\"subtasks\"]:\n",
        "                if \"id\" not in subtask or \"desc\" not in subtask:\n",
        "                    raise ValueError(\"Subtask missing required fields: 'id' or 'desc'\")\n",
        "\n",
        "            return plan\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Failed to parse JSON response. Attempting recovery...\")\n",
        "            try:\n",
        "                # Fallback: Extract JSON from text\n",
        "                json_match = re.search(r'\\{.*\\}', response_content, re.DOTALL)\n",
        "                if json_match:\n",
        "                    return json.loads(json_match.group())\n",
        "                raise\n",
        "            except:\n",
        "                print(\"Could not recover valid JSON. Using fallback plan.\")\n",
        "                return self._fallback_plan(task)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Decomposition error: {str(e)}\")\n",
        "            return self._fallback_plan(task)\n",
        "\n",
        "    def _fallback_plan(self, task):\n",
        "        \"\"\"Fallback plan for error recovery\"\"\"\n",
        "        return {\n",
        "            \"subtasks\": [\n",
        "                {\n",
        "                    \"id\": 1,\n",
        "                    \"desc\": f\"Analyze requirements: {task}\",\n",
        "                    \"dependencies\": []\n",
        "                },\n",
        "                {\n",
        "                    \"id\": 2,\n",
        "                    \"desc\": f\"Design solution architecture for {task}\",\n",
        "                    \"dependencies\": [1]\n",
        "                },\n",
        "                {\n",
        "                    \"id\": 3,\n",
        "                    \"desc\": f\"Implement core functionality for {task}\",\n",
        "                    \"dependencies\": [2]\n",
        "                },\n",
        "                {\n",
        "                    \"id\": 4,\n",
        "                    \"desc\": f\"Test and validate solution for {task}\",\n",
        "                    \"dependencies\": [3]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "class ExecutorAgent:\n",
        "    def __init__(self):\n",
        "        # Initialize Gemini model\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash-latest\",\n",
        "            temperature=0.2,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"CodeWriter\",\n",
        "                func=self.write_code,\n",
        "                description=\"Generates code for given requirements\"\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"TestRunner\",\n",
        "                func=self.run_tests,\n",
        "                description=\"Executes test cases and reports results\"\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def write_code(self, requirements: str) -> dict:\n",
        "        \"\"\"\n",
        "        Uses the LLM to generate Python code for the given requirements.\n",
        "        Returns a dict with 'code' (the Python source) and 'files' (list of filenames).\n",
        "        \"\"\"\n",
        "        prompt = (\n",
        "            f\"You are an expert Python developer. \"\n",
        "            f\"Write complete, self-contained Python code to {requirements}. \"\n",
        "            f\"Return only the code block, wrapped in triple backticks for Python.\"\n",
        "        )\n",
        "        llm_response = self.llm.invoke(prompt).content\n",
        "\n",
        "        # Extract code from triple backticks\n",
        "        match = re.search(r\"```(?:python)?\\n([\\s\\S]*?)```\", llm_response)\n",
        "        code = match.group(1) if match else llm_response\n",
        "\n",
        "        # Save code to a temp file\n",
        "        tmp_dir = tempfile.mkdtemp()\n",
        "        file_path = os.path.join(tmp_dir, \"main.py\")\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.write(code)\n",
        "\n",
        "        return {\"code\": code, \"files\": [file_path]}\n",
        "\n",
        "    def run_tests(self, code_info: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Executes the generated Python file in a sandboxed subprocess.\n",
        "        Captures pass/fail status and any stderr output.\n",
        "        \"\"\"\n",
        "        files = code_info.get(\"files\", [])\n",
        "        if not files:\n",
        "            return {\"passed\": False, \"errors\": [\"No file to execute.\"]}\n",
        "\n",
        "        file_to_run = files[0]\n",
        "\n",
        "        # Run the Python file\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"python\", file_to_run],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=10\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                return {\"passed\": True, \"errors\": []}\n",
        "            else:\n",
        "                return {\"passed\": False, \"errors\": [result.stderr.strip()]}\n",
        "        except Exception as e:\n",
        "            return {\"passed\": False, \"errors\": [str(e)]}\n",
        "\n",
        "class ReviewerAgent:\n",
        "    def __init__(self):\n",
        "        # Initialize Gemini model\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash-latest\",\n",
        "            temperature=0.2,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "\n",
        "    def analyze(self, code_result: dict, test_result: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Analyze the generated code and test results, providing feedback and critical issues.\n",
        "        Returns a dict with:\n",
        "          - 'feedback': A summary string of overall quality\n",
        "          - 'critical_issues': A list of any issues that must be addressed\n",
        "        \"\"\"\n",
        "\n",
        "        code = code_result.get(\"code\", \"\")\n",
        "        tests = test_result.get(\"errors\", []) or []\n",
        "        tests_output = \"\\n\".join(tests) if tests else \"All tests passed.\"\n",
        "\n",
        "        # Construct LLM prompt requesting JSON output\n",
        "        prompt = (\n",
        "            \"You are a senior software engineer and code reviewer. \"\n",
        "            \"Review the following Python code snippet and its test results. \"\n",
        "            \"Provide a JSON object with two keys:\\n\"\n",
        "            \" - feedback: A concise summary of overall code quality.\\n\"\n",
        "            \" - critical_issues: A list of strings describing any bugs, logical errors, \"\n",
        "            \"or style issues that should be fixed.\\n\\n\"\n",
        "            f\"Code:\\n```python\\n{code}\\n```\\n\\n\"\n",
        "            f\"Test Results:\\n{tests_output}\\n\\n\"\n",
        "            \"Output JSON only.\"\n",
        "        )\n",
        "\n",
        "        llm_response = self.llm.invoke(prompt).content\n",
        "\n",
        "        # Extract JSON from LLM response\n",
        "        try:\n",
        "            # Sometimes the model wraps JSON in code fences\n",
        "            json_str = re.search(r\"\\{[\\s\\S]*\\}\", llm_response).group(0)\n",
        "            result = json.loads(json_str)\n",
        "            # Ensure keys exist\n",
        "            feedback = result.get(\"feedback\", \"\")\n",
        "            critical_issues = result.get(\"critical_issues\", [])\n",
        "        except Exception:\n",
        "            # Fallback: return the raw response as feedback\n",
        "            feedback = llm_response.strip()\n",
        "            critical_issues = []\n",
        "\n",
        "        return {\n",
        "            \"feedback\": feedback,\n",
        "            \"critical_issues\": critical_issues\n",
        "        }\n",
        "\n",
        "class Orchestrator:\n",
        "    def __init__(self):\n",
        "        self.planner = PlannerAgent()\n",
        "        self.executor = ExecutorAgent()\n",
        "        self.reviewer = ReviewerAgent()\n",
        "        # Add a place to collect code for each subtask\n",
        "        self._generated = []\n",
        "\n",
        "    def execute_project(self, user_request):\n",
        "        # Agent collaboration workflow\n",
        "        plan = self.planner.decompose(user_request)\n",
        "\n",
        "        # Check if the plan is valid before proceeding\n",
        "        if not plan or \"subtasks\" not in plan or not isinstance(plan[\"subtasks\"], list):\n",
        "            print(\"Planner returned an invalid plan.\")\n",
        "            return \"Project execution failed: Invalid plan.\"\n",
        "\n",
        "        # Process each subtask\n",
        "        for subtask in plan[\"subtasks\"]:\n",
        "            # Skip invalid subtasks\n",
        "            if not isinstance(subtask, dict) or \"desc\" not in subtask:\n",
        "                print(f\"Skipping invalid subtask: {subtask}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nüîß Processing subtask {subtask['id']}: {subtask['desc']}\")\n",
        "\n",
        "            # Generate code for this subtask\n",
        "            code_result = self.executor.write_code(subtask[\"desc\"])\n",
        "\n",
        "            # Check if code_result is valid\n",
        "            if not code_result or \"code\" not in code_result:\n",
        "                print(f\"Executor failed to generate code for subtask: {subtask['desc']}\")\n",
        "                continue\n",
        "\n",
        "            # Store the generated code\n",
        "            self._generated.append({\n",
        "                \"id\": subtask[\"id\"],\n",
        "                \"desc\": subtask[\"desc\"],\n",
        "                \"code\": code_result[\"code\"]\n",
        "            })\n",
        "\n",
        "            print(\"üß™ Running tests...\")\n",
        "            test_result = self.executor.run_tests(code_result)\n",
        "\n",
        "            # Check if test_result is valid\n",
        "            if not test_result or \"passed\" not in test_result:\n",
        "                print(f\"Executor failed to run tests for subtask: {subtask['desc']}\")\n",
        "                continue\n",
        "\n",
        "            if not test_result[\"passed\"]:\n",
        "                print(\"üîç Reviewing failed tests...\")\n",
        "                review = self.reviewer.analyze(code_result, test_result)\n",
        "                print(f\"üìù Review feedback: {review.get('feedback', 'No feedback')}\")\n",
        "                if review.get(\"critical_issues\"):\n",
        "                    print(f\"‚ùå Critical issues: {review['critical_issues']}\")\n",
        "            else:\n",
        "                print(\"‚úÖ Tests passed\")\n",
        "\n",
        "        # Build the final output after processing all subtasks\n",
        "        if not self._generated:\n",
        "            return \"No code generated for any subtasks\"\n",
        "\n",
        "        final_output = \"\\n\\n\".join(\n",
        "            f\"# Subtask {item['id']}: {item['desc']}\\n\\n\"\n",
        "            f\"{item['code']}\\n\"\n",
        "            for item in self._generated\n",
        "        )\n",
        "\n",
        "        return final_output\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    system = Orchestrator()\n",
        "    response = system.execute_project(\"Design a production-ready Flask API for a stock market prediction model. Include endpoints to submit historical data, get predictions, and retrieve model performance metrics. Use best practices for input validation and error handling\")\n",
        "    print(f\"\\n‚úÖ {response}\")"
      ]
    }
  ]
}